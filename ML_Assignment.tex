\documentclass[11pt]{article}

\usepackage{amsthm,amsmath,amssymb}
\usepackage{enumerate}
\usepackage{algorithm,algorithmic}
\usepackage{bbm,bm}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{fullpage}
\usepackage{wrapfig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% operators
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ExpChar}{\mathbb{E}}
\DeclareMathOperator*{\Exp}{\ExpChar}   % expectation
%\newcommand{\Exp}[1]{\mathbf{E}#1}
% I do not like the large space between the argument in brackets and I which is put
% there by \DeclareMathOperator
\DeclareMathOperator*{\Err}{Err}
\DeclareMathOperator*{\VC}{VC-dim}
\DeclareMathOperator*{\Ldim}{L-dim}
\DeclareMathOperator{\Rademacher}{\mathcal{R}}
\DeclareMathOperator{\SRademacher}{\mathcal{SR}}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\conv}{conv}
%\newcommand{\indicator}[1][]{\mathbb{I}_{#1}}
\newcommand{\indicator}[1]{\mathbb{I}_{#1}}

\DeclareMathOperator*{\rows}{\#rows}
\DeclareMathOperator*{\shattered}{\#shattered}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\grad}{\nabla}
\DeclareMathOperator{\tr}{tr}
%\DeclareMathOperator{\minimize}{minimize}
\DeclareMathOperator{\subject}{\text{subject to}}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\R}{\mathbb{R}}
\newcommand{\hR}{\widehat{R}}

% Csaba likes these:
\newcommand{\EE}[1]{\Exp\left[#1\right]}
\newcommand{\EEtext}[1]{\Exp[#1]}
\newcommand{\EEi}[2]{\ExpChar_{#1}\left[#2\right]}
\newcommand{\EEtexti}[2]{\ExpChar_{#1}[#2]}
\newcommand{\Prob}[1]{\Pr\left(#1\right)}
\newcommand{\Probsq}[1]{\Pr\left[#1\right]}
\newcommand{\VVar}[1]{\Var\left[#1\right]}
\newcommand{\one}[1]{\indicator{\{#1\}}}
\newcommand{\beqan}{\begin{eqnarray*}}
\newcommand{\eeqan}{\end{eqnarray*}}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ra}{\rightarrow}
\newcommand{\YY}{{\mathcal Y}}
\newcommand{\X}{{\mathcal X}}
\renewcommand{\H}{{\mathcal H}}
\newcommand{\D}{{\mathcal D}}
\newcommand{\U}{U}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\Algset}{{\cal A}} % Algorithm set
\newcommand{\FF}{{\cal F}} % Expert set
\newcommand{\real}{\mathbb{R}}
\newcommand{\hp}{\widehat{p}}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\ipgrow}[1]{\left\langle #1 \right\rangle}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\ttop}{^{\kern-1.5pt\top}}
\newcommand{\ww}{\widetilde w}
\newcommand{\wl}{\widetilde\ell}
\newcommand{\eqdef}{\stackrel{{\rm def}}{=}}
\newcommand{\swe}{\sigma}
\newcommand{\Swe}{\widehat{\sigma}}
\newcommand{\cset}[2]{\left\{\,#1\,:\,#2\, \right\}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\hL}{\widehat{L}}
\newcommand{\reg}{\mathcal{R}}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}

\newcommand{\tell}{\tilde{\ell}}

% theorems/definitions
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}



\newcommand{\bookbox}[1]{
    \par\noindent
    \begin{center}
    \framebox[.94\textwidth]{
    \begin{minipage}{0.82 \textwidth}{#1}
    \end{minipage} }
    \end{center}
    \par\noindent }

\newcommand{\bookboxt}[2]{
    \bookbox{
      \begin{center}
        \sc #1
      \end{center} 
    % \medskip 
      #2 }
}

\newtheorem{problem}{Problem}

\newcommand{\point}[1]{\hfill {\bf [#1 points]}} 

\newif\ifhints
\hintsfalse
%\hintstrue

\newcommand{\hint}[1]{
\ifhints
\par
\noindent\emph{Hint: } #1
\fi
}

\newif\ifsolutions
%\solutionsfalse
\solutionstrue

\newcommand{\solution}[1]{
\ifsolutions
\smallskip
\par
\noindent\emph{Solution: } #1
\fi
}

%\usepackage[disable]{todonotes}
\usepackage{todonotes}
\newcommand{\todoa}[2][]{\todo[color=red!20,#1]{A: #2}}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\rhead{CID:\hspace{3cm}}
\lhead{username \tiny{(College login e.g. me11234)}:}
%\headsep{5cm}
\setlength{\headsep}{1cm}
\begin{document}
%\thispagestyle{empty}
%\addtolength{\topmargin}{5cm}
\bigskip
\bigskip

\begin{center}
%{\Large \textbf{EE3-23: Assigment}} \\
%9 March, 2018


%\bigskip
%\bigskip
\bigskip

{\Huge\textbf{EE3-23: Assigment 2018}} \\

\end{center}

%s\bigskip

Provide answers to all 5 problems given on the next page. Maximum number of possible mark points are given below each problem. The total adds up to  {\bf[20 points]}, which will be added to the marks obtained from the exam { [80 points]}.


 Much of reporting in the domain of Machine Learning is done using Latex, which is a document preparation system for high-quality typesetting available for Linux, Windows and Mac. Present your solution within this Latex file i.e. edit the source file and compile. The solutions must be clearly presented with equations or figures if appropriate but also succinctly.   Extensive copying of lecture material indicates weak understanding and poor effort, so points will be deducted for including irrelevant information. Use mathematical formulations as much as possible, rather than verbal descriptions. 
 
{\bf Do not:} 1) exceed { 4 pages}, 2) remove the questions, 3) change the document style. 

\bigskip
{\bf Submission deadline:} 16 November 2018, 23:59 \\

{\bf Declaration:}\\
I,	$<$YOUR NAME$>$, pledge that this assignment is completely my own work, and that I did not take, borrow or steal work from any other person, and that I did not allow any other person to use, have, borrow or steal portions of my work. I understand that if I violate this honesty pledge, I am subject to disciplinary action pursuant to the appropriate sections of Imperial College London.


\newpage
%\thispagestyle{empty}



%\bigskip



%\bigskip
%\bigskip



\begin{problem}[Overfitting]\em
Explain the relation between the training error, test error, and the model complexity. 

\point{3}
\solution{
While training error always decreases as model complexity increases, test error behaves differently. When model complexity is relatively low, test error will decrease just like training error, but only up to a certain point. From then on test error will start increasing if the model complexity keeps increasing. When the complexity goes too high, it results in what is called overfitting, where the model performs very well in training data but has terrible performance in test data. In other words, the in-sample error poorly tracks the out-sample error, which is undesirable. In the other case which is called underfitting, the model complexity is too low that it performs badly in both training and test data.
}
\end{problem}

%\bigskip

\begin{problem} [SVM]\em
Consider a binary classification problem where the training set is not linearly separable. Given this knowledge, can you argue which of the SVM based methods is most (and least) promising, and in what scenarios? 

\point{4} 
\solution{
[Present your solution here...]

}
\end{problem}

%\bigskip

\hspace{-0.041\textwidth}
\begin{minipage}{0.55\textwidth}
\begin{problem}[MLP] \em
A network of three neurons with  ReLU activations has been trained with stochastic gradient descent, L2 loss, and learning rate of $0.1$.  The current weights are given in the figure. Forward propagate  training example $(x_1,x_2)=(16,8)$ with label $y=5$, calculate outputs of  all neurons, then  calculate  the update of weight $w_{bc}$ using backpropagation. Explain your calculations with appropriate formulas. 

\point{4}
\end{problem}
\end{minipage}  
\hspace{0.09\textwidth}
\begin{minipage}{0.4\textwidth}
  \includegraphics[width=\textwidth]{nntest.png} 
\end{minipage}  

\solution{
    $$\theta(s) = S_+ = max\big\{0, s\big\}, \eta = 0.1$$ 
    forward propogation:
    $$a = 0 + x_1*w_{1a} + x_2*w_{2a} = 0 + 16*0.125 + 8*0.75 = 8$$
    $$x^1_1 = \theta(a) = max\big\{0,8\big\} = 8$$
    $$b = 0 + x_1*w_{1b} + x_2*w_{2b} = 0 + 16*0.5 + 8*0.25 = 10$$
    $$x^1_2 = \theta(b) = max\big\{0,10\big\} = 10$$
    $$c = 0 + x^1_1*w_{ac} + x^1_2*w_{bc} = 0 + 8*0.5 + 10*0.1 = 5$$
    $$h(x) = x^2_c = \theta(c) = max\big\{0,5\big\} = 5$$
    backpropogation:
    $$\frac{\partial e(\textbf{w})}{\partial w_{bc}} = \frac{\partial e(\textbf{w})}{\partial c}*\frac{\partial c}{\partial w_bc}$$
    $$\frac{\partial c}{\partial w_bc}=x^1_2 = 10, \frac{\partial e(\textbf{w})}{\partial c} = \delta c$$
    $$e(\textbf w)=\frac{1}{2}\big(\theta(c)-y\big)^2$$
    $$\theta'(c) = 
    \begin{cases}
        0\quad(c<0)\\
        1\quad(c>=0)
    \end{cases}$$
    $$\delta c = \frac{\partial e(\textbf w)}{\partial \theta(c)}*\frac{\partial \theta(c)}{\partial c} = \big(\theta(c) - y\big)*1 = 5 - 7 = -2$$
    $$\frac{\partial e(\textbf w)}{w_{bc}} = -2 * 10 = -20$$ 
    $$w'_{bc} = w_{bc} - \eta * \frac{\partial e(\textbf w)}{w_{bc}} = 0.1 - 0.1 * (-20) = 2.1$$
}

\begin{problem}[Performance]\em
As the data science expert, you are asked to give a recommendation about a classification algorithm designed to predict, under the zero-one error, if a student gets a job offer within 1 month of graduation. As due diligence, you want to estimate the performance of the predictor. What assumptions do you have to make? Propose  vector representation of student transcripts to be used as input data. What is required if you want to know the performance of the predictor with an absolute error of at most $0.02$ with probability at least $0.95$, and given that your hypothesis class size has $\mathcal{|H|}=100$? 

\point{3}

\solution{ 

[Present your solution here...]

}
\end{problem}


\begin{problem}[Regression] \em 
You are further asked to design a method to predict the starting  salary of a student. You are given a spreadsheet with 100 rows each containing a student's final average mark and his starting salary.
 You believe that a polynomial $h(x)$ can be the right approach to address this problem.
\begin{enumerate}[(a)]
\item One of your colleagues recommends that since a degree 100 polynomial could perfectly fit your data points, $h(x)$ should be of degree 100. Is your colleague right or wrong? Explain your answer.

\point{2}
\solution{ 

[Present your solution here...]

}
\item Formalize this ML problem. Recommend a method that favours lower order polynomials. 

\point{4}
\solution{

[Present your solution here...]
}
\end{enumerate}


\end{problem}




\end{document}
